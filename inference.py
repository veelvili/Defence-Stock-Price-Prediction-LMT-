# -*- coding: utf-8 -*-
"""inference.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vG1ovKYS9SLtbSNhP0UDW8n6MZaK9xfi
"""

import os, json, joblib, numpy as np, pandas as pd, yfinance as yf
from xgboost import XGBRegressor
from tensorflow.keras.models import load_model

# ------------ Artifacts & Config ------------
ART_DIR = os.getenv("ART_DIR", "artifacts")
CFG_PATH = os.path.join(ART_DIR, "config.json")
with open(CFG_PATH) as f:
    CFG = json.load(f)

LOOKBACK     = int(CFG["lookback"])
FEATURE_COLS = CFG["feature_cols"]
GEO_COLS     = CFG["geo_cols"]          # expects: ["GPRD","GPRD_ACT","GPRD_THREAT","GPRD_MA30","GPRD_MA7"]
TICKER       = CFG.get("ticker", "LMT")

scaler_X_lstm = joblib.load(os.path.join(ART_DIR, "scaler_X_lstm.joblib"))
scaler_y_lstm = joblib.load(os.path.join(ART_DIR, "scaler_y_lstm.joblib"))
lstm          = load_model(os.path.join(ART_DIR, "lstm.keras"))
rf_best       = joblib.load(os.path.join(ART_DIR, "rf.joblib"))
svr_best      = joblib.load(os.path.join(ART_DIR, "svr.joblib"))
meta          = joblib.load(os.path.join(ART_DIR, "meta.joblib"))
xgb_best      = XGBRegressor()
xgb_best.load_model(os.path.join(ART_DIR, "xgb.json"))

# geopolitics CSV path (you'll upload/replace this file via Streamlit)
GEO_PATH = os.getenv("GEO_CSV_PATH", os.path.join(ART_DIR, "geopolitical_data.csv"))

# ------------ Feature Engineering ------------
def add_technical_indicators(dfin: pd.DataFrame) -> pd.DataFrame:
    df = dfin.copy()
    # RSI(14)
    delta = df["Close"].diff()
    gain = (delta.where(delta > 0, 0)).fillna(0)
    loss = (-delta.where(delta < 0, 0)).fillna(0)
    avg_gain = gain.ewm(span=14, adjust=False).mean()
    avg_loss = loss.ewm(span=14, adjust=False).mean()
    rs = avg_gain / avg_loss
    df["RSI"] = 100 - (100 / (1 + rs))

    # MACD (12,26,9)
    exp1 = df["Close"].ewm(span=12, adjust=False).mean()
    exp2 = df["Close"].ewm(span=26, adjust=False).mean()
    df["MACD"] = exp1 - exp2
    df["MACD_signal"] = df["MACD"].ewm(span=9, adjust=False).mean()

    # Stochastic (14,3)
    low_14 = df["Low"].rolling(window=14).min()
    high_14 = df["High"].rolling(window=14).max()
    df["Stoch_K"] = ((df["Close"] - low_14) / (high_14 - low_14)) * 100
    df["Stoch_D"] = df["Stoch_K"].rolling(window=3).mean()

    # ATR(14)
    high_low = df["High"] - df["Low"]
    high_prev = np.abs(df["High"] - df["Close"].shift())
    low_prev  = np.abs(df["Low"]  - df["Close"].shift())
    tr = pd.concat([high_low, high_prev, low_prev], axis=1).max(axis=1)
    df["ATR"] = tr.rolling(window=14).mean()

    # OBV
    df["OBV"] = (np.sign(df["Close"].diff()) * df["Volume"]).fillna(0).cumsum()
    return df

def _standardize_date_col(df: pd.DataFrame) -> pd.DataFrame:
    if "Date" not in df.columns:
        for c in df.columns:
            if c.lower() == "date":
                df = df.rename(columns={c: "Date"})
                break
    if "Date" not in df.columns:
        raise ValueError("Geopolitics CSV must contain a 'Date' column.")
    df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
    df = df.dropna(subset=["Date"]).sort_values("Date")
    return df

def load_geopolitics_csv(path: str = GEO_PATH) -> pd.DataFrame | None:
    if not os.path.exists(path):
        return None
    geo_raw = pd.read_csv(path)
    geo_raw = _standardize_date_col(geo_raw)
    missing = [c for c in GEO_COLS if c not in geo_raw.columns]
    if missing:
        raise ValueError(f"Geopolitics CSV missing required columns: {missing} "
                         f"(expected: {GEO_COLS})")
    geo = geo_raw[["Date"] + GEO_COLS].copy()
    geo[GEO_COLS] = geo[GEO_COLS].ffill()
    return geo

def merge_geopolitics(price_df: pd.DataFrame, geo_df: pd.DataFrame | None) -> pd.DataFrame:
    df = price_df.copy()
    if geo_df is None:
        # if you want to hard-require geo data, raise here instead:
        # raise RuntimeError("Geopolitics CSV not provided yet.")
        for c in GEO_COLS:
            if c not in df.columns:
                df[c] = np.nan
        return df
    out = pd.merge(df, geo_df, on="Date", how="left", suffixes=("", "_geo"))
    for c in GEO_COLS:
        if f"{c}_geo" in out.columns:
            out[c] = out[f"{c}_geo"].combine_first(out.get(c))
            out.drop(columns=[f"{c}_geo"], inplace=True, errors="ignore")
    out[GEO_COLS] = out[GEO_COLS].ffill()
    return out

def fetch_prices(period_days: int = 800, ticker: str = TICKER) -> pd.DataFrame:
    # Adjusted OHLC from yfinance
    df = yf.download(ticker, period=f"{max(365, period_days)}d",
                     interval="1d", auto_adjust=True, progress=False).reset_index()
    df.columns = df.columns.get_level_values(0)
    df = df.dropna().copy()
    return df

def make_sequences(X_scaled: np.ndarray, lookback: int) -> np.ndarray:
    X_seq = []
    for i in range(lookback, len(X_scaled)):
        X_seq.append(X_scaled[i - lookback:i, :])
    return np.array(X_seq)

# ------------ Core Predictors ------------
def predict_series(days_back: int = 250, ticker: str = TICKER) -> pd.DataFrame:
    """
    Returns a DataFrame indexed by Date with columns:
    ['Actual','LSTM','RF','XGB','SVR','STACKED'] for the last N days.
    """
    # fetch data
    period_needed = LOOKBACK + days_back + 200
    price = fetch_prices(period_days=period_needed, ticker=ticker)
    price["Date"] = pd.to_datetime(price["Date"])
    price = add_technical_indicators(price)

    # geopolitics
    geo_df = load_geopolitics_csv(GEO_PATH)
    full = merge_geopolitics(price, geo_df)
    full = full.dropna().reset_index(drop=True)

    # features/target
    X_all = full[FEATURE_COLS].copy()
    y_all = full["Close"].copy()
    dates = full["Date"].copy()

    # LSTM
    X_s = scaler_X_lstm.transform(X_all)
    X_seq = make_sequences(X_s, LOOKBACK)
    lstm_pred_s = lstm.predict(X_seq, verbose=0).ravel()
    lstm_pred = scaler_y_lstm.inverse_transform(lstm_pred_s.reshape(-1,1)).ravel()
    lstm_dates = dates.iloc[LOOKBACK:]

    # Classical models (trained on same-day features in your pipeline)
    rf_pred  = pd.Series(rf_best.predict(X_all.iloc[LOOKBACK:]),  index=lstm_dates, name="RF")
    xgb_pred = pd.Series(xgb_best.predict(X_all.iloc[LOOKBACK:]), index=lstm_dates, name="XGB")
    svr_pred = pd.Series(svr_best.predict(X_all.iloc[LOOKBACK:]), index=lstm_dates, name="SVR")
    lstm_ser = pd.Series(lstm_pred, index=lstm_dates, name="LSTM")
    y_ser    = pd.Series(y_all.iloc[LOOKBACK:].values, index=lstm_dates, name="Actual")

    stack_df = pd.DataFrame({"LSTM": lstm_ser, "RF": rf_pred, "XGB": xgb_pred, "SVR": svr_pred}).dropna()
    y_stack  = y_ser.reindex(stack_df.index)
    stacked  = meta.predict(stack_df[["LSTM","RF","XGB","SVR"]])
    stacked_ser = pd.Series(stacked, index=stack_df.index, name="STACKED")

    # trim to days_back
    out = pd.concat([y_stack, lstm_ser, rf_pred, xgb_pred, svr_pred, stacked_ser], axis=1).dropna()
    if days_back > 0:
        out = out.iloc[-days_back:]
    return out

def meta_coefficients() -> dict:
    return {k: float(v) for k, v in zip(["LSTM","RF","XGB","SVR"], meta.coef_)}